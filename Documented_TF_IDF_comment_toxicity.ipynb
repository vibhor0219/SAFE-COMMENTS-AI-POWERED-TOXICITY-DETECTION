{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importing libraries**"
      ],
      "metadata": {
        "id": "fcJ05giQyqMh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKVA2or9ljzT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mount Google Drive and Read CSV Files**"
      ],
      "metadata": {
        "id": "bCIMTSUEyyXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# After mounting, you can access files like this:\n",
        "df_train = pd.read_csv('/content/drive/My Drive/Comment_toxic/jigsaw dataset/train.csv')\n",
        "df_test = pd.read_csv('/content/drive/My Drive/Comment_toxic/jigsaw dataset/test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BO8y-dd1l7ix",
        "outputId": "a4becf0c-b860-4207-b46f-d7ab0b3224b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Concatenate DataFrames and Convert Comment Text to Lowercase**"
      ],
      "metadata": {
        "id": "kihxldt6zUpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df_train, df_test])\n",
        "df_train['comment_text'] = df_train['comment_text'].str.lower()\n",
        "df_test['comment_text'] = df_test['comment_text'].str.lower()\n",
        "df_train.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxu7Y4NSoZVN",
        "outputId": "d46c98b5-53a4-439c-d1c8-038edb72c518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 159571 entries, 0 to 159570\n",
            "Data columns (total 8 columns):\n",
            " #   Column         Non-Null Count   Dtype \n",
            "---  ------         --------------   ----- \n",
            " 0   id             159571 non-null  object\n",
            " 1   comment_text   159571 non-null  object\n",
            " 2   toxic          159571 non-null  int64 \n",
            " 3   severe_toxic   159571 non-null  int64 \n",
            " 4   obscene        159571 non-null  int64 \n",
            " 5   threat         159571 non-null  int64 \n",
            " 6   insult         159571 non-null  int64 \n",
            " 7   identity_hate  159571 non-null  int64 \n",
            "dtypes: int64(6), object(2)\n",
            "memory usage: 9.7+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KHHGT5yqHwK",
        "outputId": "adec1b22-3a73-447d-b728-784af873ce26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(312735, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function to Remove Special Characters using Regular expression**"
      ],
      "metadata": {
        "id": "EcIW51kDzwEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    text = re.sub(r'http\\S+', ' ', text )\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'\\d', ' ', text)  # Corrected line\n",
        "    return text\n",
        "\n",
        "df_train['comment_text'] = df_train['comment_text'].apply(remove_special_characters)\n",
        "df_test['comment_text'] = df_test['comment_text'].apply(remove_special_characters)\n",
        "\n",
        "print(df_train['comment_text'].head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9FORTg9p9U9",
        "outputId": "93deef7c-a270-4cce-a70c-5e3f05311ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    explanation why the edits made under my userna...\n",
            "1    d aww he matches this background colour i m se...\n",
            "2    hey man i m really not trying to edit war it s...\n",
            "3    more i can t make any real suggestions on impr...\n",
            "4    you sir are my hero any chance you remember wh...\n",
            "5    congratulations from me as well use the tools ...\n",
            "6         cocksucker before you piss around on my work\n",
            "7    your vandalism to the matt shirvington article...\n",
            "8    sorry if the word nonsense was offensive to yo...\n",
            "9    alignment on this subject and which are contra...\n",
            "Name: comment_text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenize Text**"
      ],
      "metadata": {
        "id": "pCxMvpWh0FjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk                     #Imports the Natural Language Toolkit (NLTK), a library for natural language processing (NLP) in Python.\n",
        "nltk.download('punkt')          #Downloads the 'punkt' tokenizer models, which are used for tokenizing text into sentences or words.\n",
        "from nltk import word_tokenize  #Imports the word_tokenize function from NLTK, which tokenizes a string into words.\n",
        "\n",
        "df_train['word_tokens'] = df_train['comment_text'].apply(word_tokenize) #Applies the word_tokenize function to each entry in the comment_text column of\n",
        "                                                                        #df_train and stores the resulting list of word tokens\n",
        "                                                                        #in a new column called word_tokens.\n",
        "\n",
        "df_test['word_tokens'] = df_test['comment_text'].apply(word_tokenize)   #Applies the word_tokenize function to each entry in the comment_text column of\n",
        "                                                                        #df_test and stores the resulting list of word tokens in a new column\n",
        "                                                                        #called word_tokens."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEQ1AEMhqtzu",
        "outputId": "fcb701e7-9cad-4bea-94a5-bb7aebb2f9db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import the train_test_split Function and Split the Data into training dataset and validation dataset**"
      ],
      "metadata": {
        "id": "tsZv37tw1eGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, valid = train_test_split(df_train, train_size=0.8, random_state=42)"
      ],
      "metadata": {
        "id": "IDpwHn7XrsQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF Vectorization**"
      ],
      "metadata": {
        "id": "ItvO6kYY4GAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk import word_tokenize\n",
        "\n",
        "# Define the TfidfVectorizer with specified parameters\n",
        "vec = TfidfVectorizer(ngram_range=(1, 2),     #Considers unigrams and bigrams (1-word and 2-word combinations).\n",
        "                      min_df=3,               #Ignores terms that appear in fewer than 3 documents.\n",
        "                      max_df=0.9,             #Ignores terms that appear in more than 90% of the documents.\n",
        "                      strip_accents='unicode',#Removes accents from characters.\n",
        "                      use_idf=1,              #Enables the use of inverse document frequency weighting.\n",
        "                      smooth_idf=1,           #Applies smoothing to the IDF weights by adding one to the document frequencies.\n",
        "                      sublinear_tf=1,         #Applies sublinear term frequency scaling (using the logarithm of term frequency).\n",
        "                      binary=1,               #If true, all non-zero term counts are set to 1 (boolean \"occurrence\" instead of \"frequency\").\n",
        "                      stop_words='english')   #Removes common English stop words.\n",
        "\n",
        "\n",
        "# Transform the training, validation, and test data\n",
        "trn_term_doc = vec.fit_transform(df_train['comment_text'])    #Fits the TfidfVectorizer to the training data and transforms it into a TF-IDF matrix.\n",
        "val_term_doc = vec.transform(valid['comment_text'])           #Transforms the validation data using the same vectorizer fitted on the training data.\n",
        "test_term_doc = vec.transform(df_test['comment_text'])        #Transforms the test data using the same vectorizer fitted on the training data.\n",
        "x = trn_term_doc                                              #Stores the TF-IDF matrix for the training data.\n",
        "val_x = val_term_doc                                          #Stores the TF-IDF matrix for the validation data."
      ],
      "metadata": {
        "id": "ddWDS-Osr0PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing** **Libraries**"
      ],
      "metadata": {
        "id": "8wf4vPLvxbTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "id": "cq3j6fA6xWse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the Probability Function**"
      ],
      "metadata": {
        "id": "D1akWvGoxtUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 1e-9  # Define epsilon as a small positive constant\n",
        "# Define a function to calculate the probability of each word given a specific class (toxic or non-toxic)\n",
        "def probability(y_i, y):\n",
        "    # Sum the occurrences of each word in comments labeled with y_i (1 for toxic, 0 for non-toxic)\n",
        "    occurences = x[y == y_i].sum(0)\n",
        "    # Add a smoothing factor of 1 to avoid division by zero and handle words not present in some classes\n",
        "    return (occurences + 1) / ((y == y_i).sum() + 1)"
      ],
      "metadata": {
        "id": "2yjVFi2pxYAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the Logistic Model Function**"
      ],
      "metadata": {
        "id": "Pbdyj-K0x2yM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(y):\n",
        "    # Convert the target labels to a numpy array\n",
        "    y = y.values\n",
        "    # Calculate the log-ratio of probabilities of each word being toxic vs. non-toxic\n",
        "    loga = np.log((probability(1, y) + epsilon) / (probability(0, y) + epsilon) )\n",
        "    # Multiply the input features by the log-ratio to incorporate the information about word toxicity\n",
        "    x_loga = x.multiply(loga)\n",
        "    # Initialize a logistic regression model with specified hyperparameters\n",
        "    model = LogisticRegression(C=1.0, penalty='l2', solver='liblinear', max_iter=100, random_state=42)\n",
        "    # Fit the model to the modified input features and target labels\n",
        "    return model.fit(x_loga, y), loga"
      ],
      "metadata": {
        "id": "BmnejXsDxiqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Classes and Labels**"
      ],
      "metadata": {
        "id": "XdST7J82yCTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "train_labels = df_train.drop(['comment_text'], axis = 1)\n",
        "valid_labels = valid.drop(['comment_text'], axis = 1)"
      ],
      "metadata": {
        "id": "RmMf32-ux1f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Models and Evaluate**"
      ],
      "metadata": {
        "id": "X9WOAiriyShe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store ROC AUC scores for each class\n",
        "model = {}\n",
        "ROC_AUC_Scores = {}\n",
        "for i, col in enumerate(classes):\n",
        "    print(col)\n",
        "\n",
        "    # Train model for current class\n",
        "    model_trained, loga = get_model(train_labels[col])\n",
        "    model[col] = (model_trained, loga)\n",
        "    # Make predictions on validation set\n",
        "    preds = model_trained.predict(val_x.multiply(loga)).reshape(-1, 1)\n",
        "\n",
        "    # Calculate ROC AUC score for current class and store it\n",
        "    roc_auc = roc_auc_score(valid_labels[col], preds)\n",
        "    ROC_AUC_Scores[col] = roc_auc\n",
        "    # Print ROC AUC scores for each class\n",
        "for col, roc_auc in ROC_AUC_Scores.items():\n",
        "    print(f\"ROC AUC for class: '{col}': {roc_auc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fhTyttFx8K0",
        "outputId": "d37f4c6f-fc2f-42a5-cddf-6d231e9c24c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "toxic\n",
            "severe_toxic\n",
            "obscene\n",
            "threat\n",
            "insult\n",
            "identity_hate\n",
            "ROC AUC for class: 'toxic': 0.882298818964349\n",
            "ROC AUC for class: 'severe_toxic': 0.8593699620003561\n",
            "ROC AUC for class: 'obscene': 0.9121859131542873\n",
            "ROC AUC for class: 'threat': 0.9661534041186063\n",
            "ROC AUC for class: 'insult': 0.8711052432334527\n",
            "ROC AUC for class: 'identity_hate': 0.8808416950158198\n"
          ]
        }
      ]
    }
  ]
}
